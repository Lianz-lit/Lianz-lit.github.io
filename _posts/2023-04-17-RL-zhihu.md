---
layout:       post
title:        "强化学习"
author:       "Lianz"
header-style: text
catalog:      true
tags:
    - RL
    - zhihu
---

> 根据知乎用户[@heaven](https://www.zhihu.com/people/yu-xin-hang-22)的系列博客整理（<https://www.zhihu.com/people/yu-xin-hang-22/posts?page=4>）

**目录**

[toc]



# 1.1 什么是强化学习

<img src="https://pic4.zhimg.com/v2-8c9ba4b64d6daa539e786c6e21f519c7_r.jpg" alt="img" style="zoom: 67%;" />

  强化学习主要是属于机器学习范畴的一类问题，通常会为这类问题定义以下几个元素：

- 智能体 Agent
- 环境 Environment
- 动作 Action
- 状态 State
- 奖励 Reward

  这些元素之间的关系可以由下图表示：

<img src="https://pic3.zhimg.com/v2-4ac109628bfd404b3fdb4cf1a166cb12_r.jpg" alt="img" style="zoom:50%;" />

最终的目的是训练智能体选取合适的策略来最大化奖励，策略即面对状态该如何采取动作。

需要注意的是：
		当我们提起“强化学习”的时候，往往会用到“深度学习”的方法，并将其称作“深度强化学习”。
		但我们要注意不要混淆二者的含义，要清楚“强化学习”是一类问题，而“深度学习”是一类方法。

# 1.2 强化学习的基本思想

有监督学习和无监督学习：基于已有的数据，去学习数据的分布或蕴含的其他重要信息。
强化学习：不是基于已有数据，而是对一个环境进行学习；且目标不是数据中的信息，而是寻找能在环境中取得更多奖励的方法。

- 监督学习的目标——“弄清楚环境是什么样的”
- 强化学习的目标——“在这个环境中生活得更好”

**强化学习算法主要涉及到两个组成部分：其一是通过与环境交互产生大量的数据，**
															     **其二是利用这些数据去求解最佳策略**

设计强化学习算法时要考虑：计算量；数据效率（产生数据所需的成本）；训练效率（求解最优策略的效率）


## （1）从环境中产生数据

监督学习—— “拥有数据”意味着拥有环境中随机产生的数据。 
强化学习——“拥有环境”意味着可以自主选择与环境交互的方式，从环境中产生我们需要的数据，比随机产生的训练数据包含更多的价值
					   使探索利用平衡发挥作用

越学越强（强化学习）：
对智能体进行初步训练—》有了初步的分辨能力之后—》判断并产生相对更加“重要”的数据
然后用这些数据优化自身的策略—》更准确地判断哪些数据更加“重要”—》更有针对性地从环境中产生更加有价值的数据，
进一步优化自身的策略

​		强化学习的目标是寻找能够在环境中取得较多奖励的策略。如果仅仅依靠给定的数据是难以求解出很好的策略的。只有拥有可交互的环境，才能充分验证我们策略的有效性。

模仿学习（imitation learning）：将强化学习问题转换为有监督学习问题。但实际应用效果很差。
原因——有监督学习目标为优化预测值和真实值的误差。强化学习中，假设训练出来的模型有误差，会导致实际采取的动作有误差，使得			  状态也有误差，不断累积，智能体根本无法应对。
解决方案——DAgger（Dataset Aggregation）。用已经产生的数据进行模仿，对新产生的数据不断进行标注。

​                                                  <img src="https://pic3.zhimg.com/v2-988795f3f27434444acbd8ea5bf5b582_r.jpg" alt="img" style="zoom: 50%;" />











## （2）求解最佳策略

