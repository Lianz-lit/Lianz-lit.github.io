---
layout:       post
title:        "强化学习"
author:       "Lianz"
header-style: text
catalog:      true
tags:
    - RL
    - zhihu
---

> 根据知乎用户[@heaven](https://www.zhihu.com/people/yu-xin-hang-22)的系列博客整理（<https://www.zhihu.com/people/yu-xin-hang-22/posts?page=4>）

**目录**

[toc]



# 1.1 什么是强化学习

<img src="https://cdn.jsdelivr.net/gh/Lianz-lit/notes-pic/test_jsdelivr.png" style="zoom:50%;" />

强化学习主要是属于机器学习范畴的一类问题，通常会为这类问题定义以下几个元素：

- 智能体 Agent
- 环境 Environment
- 动作 Action
- 状态 State
- 奖励 Reward

  这些元素之间的关系可以由下图表示：

<img src="https://cdn.jsdelivr.net/gh/Lianz-lit/notes-pic/202309021956392.png" alt="img" style="zoom:50%;" />

最终的目的是训练智能体选取合适的策略来最大化奖励，策略即面对状态该如何采取动作。

需要注意的是：
		当我们提起“强化学习”的时候，往往会用到“深度学习”的方法，并将其称作“深度强化学习”。
		但我们要注意不要混淆二者的含义，要清楚“强化学习”是一类问题，而“深度学习”是一类方法。

# 1.2 强化学习的基本思想

有监督学习和无监督学习：基于已有的数据，去学习数据的分布或蕴含的其他重要信息。
强化学习：不是基于已有数据，而是对一个环境进行学习；且目标不是数据中的信息，而是寻找能在环境中取得更多奖励的方法。

- 监督学习的目标——“弄清楚环境是什么样的”
- 强化学习的目标——“在这个环境中生活得更好”

**强化学习算法主要涉及到两个组成部分：其一是通过与环境交互产生大量的数据，**
															     **其二是利用这些数据去求解最佳策略**

设计强化学习算法时要考虑：计算量；数据效率（产生数据所需的成本）；训练效率（求解最优策略的效率）


## （1）从环境中产生数据

监督学习—— “拥有数据”意味着拥有环境中随机产生的数据。 
强化学习——“拥有环境”意味着可以自主选择与环境交互的方式，从环境中产生我们需要的数据，比随机产生的训练数据包含更多的价值
					   使探索利用平衡发挥作用

越学越强（强化学习）：
对智能体进行初步训练—》有了初步的分辨能力之后—》判断并产生相对更加“重要”的数据
然后用这些数据优化自身的策略—》更准确地判断哪些数据更加“重要”—》更有针对性地从环境中产生更加有价值的数据，
进一步优化自身的策略

​		强化学习的目标是寻找能够在环境中取得较多奖励的策略。如果仅仅依靠给定的数据是难以求解出很好的策略的。只有拥有可交互的环境，才能充分验证我们策略的有效性。

模仿学习（imitation learning）：将强化学习问题转换为有监督学习问题。但实际应用效果很差。
原因——有监督学习目标为优化预测值和真实值的误差。强化学习中，假设训练出来的模型有误差，会导致实际采取的动作有误差，使得下一状态状态也有误差，不断累积，智能体根本无法应对。类似蝴蝶效应
解决方案——DAgger（Dataset Aggregation）。用已经产生的数据进行模仿，对新产生的数据不断进行标注。                                                   

<img src="https://cdn.jsdelivr.net/gh/Lianz-lit/notes-pic/v2-988795f3f27434444acbd8ea5bf5b582_r.jpg" style="zoom:50%;" />



Note：
拥有环境 VS 拥有数据。前者更好。
但实践中，缺乏可自由交互的环境，只能通过历史数据学习策略，并用于环境中——Offline Reinforcement Learning (或称为batch RL)，

## （2）求解最佳策略

- 基于价值（时间查分的思想，即time difference，TD）
- 基于策略（具有优化思想）

最优控制（optimal control）问题：求解一个环境已知的MDP
														已知环境表达式（S，AR之间的转移关系已知），求解最优策略

Summary：
与环境交互产生数据——> 通过数据求解最佳策略
需要同时兼顾二者，提升数据效率和计算效率 

# 2.1 马尔可夫决策过程

## （1）马尔可夫过程MP

对于一个马尔可夫过程（一类随机过程），**Xt+1的分布只和Xt的取值有关，和Xt之前发生过的事情无关**。一环扣一环的因果关系，形成马尔可夫链。

<img src="/Users/lit/Library/Application Support/typora-user-images/image-20230606014250177.png" alt="image-20230606014250177" style="zoom:50%;" />

<img src="/Users/lit/Library/Application Support/typora-user-images/image-20230606014406136.png" alt="image-20230606014406136" style="zoom:50%;" />

马尔可夫链的核心性质：如果已知某些条件，要求解某些表达式关于这些条件的期望，则事实上只需要距离所求事件最接近的条件就足够了。

**一个随机过程能不能用马氏过程来建模，在于能否很好地定义状态以及状态之间的转移方程。**

## （2）马尔可夫决策过程MDP

MP只有状态及状态间的内在关系，在MDP中，**增加行为$A_t$与奖励$R_t$**。

- 增加$A_t$相当于增加“主观能动性”，作为系统输入。
- 增加$R_t$相当于采取行动带来的回报，类似系统输出

使得MDP中状态间的转移关系不再是内部的，而是由状态St与行为At共同决定。

<img src="/Users/lit/Library/Application Support/typora-user-images/image-20230606015707492.png" alt="image-20230606015707492" style="zoom: 33%;" />

$R_t$由$s_t$和$a_t$共同决定，服从概率分布。

MDP中，核心目标就是求出一种选择$A_t$的方式，使获得的奖励最多。

**结束信号 $Done_t$**：代表MDP何时结束

## （3）MP与MDP对比

马氏链和MDP，最重要的性质是它们各个变量之间的关系必须满足马尔可夫性。

- MP
  - 给定条件概率$P(S_{t+1}|S_t=s_t)$，以及初始状态分布$P(S_0)$，就完全确定了马氏链的分布
  - 对于所有的s与t，可以求得所有$P(S_t=s)$
  - 对于所有马氏链轨道$(s_0,s_1,...,s_n)$ ，可以求得其发生的概率$P(\tau=(s_0,s_1,...,s_n))$

- MDP
  - 给定P、R、Done，以及初始状态分布$P(S_0)$，后续$S_t$的分布也无法确定。因为还存在**可以自由选择**的$A_t$
  - 当$P(A_t|S_t=s)$确定时，一切就确定了，可以求得$P(S_t=s)$、$P(A_t=a)$、$P(R_t=r)$的具体分布
  - 并且对于可能发生的所有轨道$(s_0,s_1,...,s_n)$ ，可以求得其发生的概率$P(\tau=(s_0,s_1,...,s_n))$

Summary：

​		MP具有“客观规律”的系统，不受外部输入影响。MDP则不断接受外部输入（$A_t$），研究MDP目的就在于选择最佳的行动。

# 2.2 MDP的分类

​		我们最终要求解的强化学习问题应该是一个环境未知且持续多步的MDP，这导致我们的算法要同时考虑两个方面的困难，即如何与环境交互产生数据、如何求解最佳策略。
​		在考虑MDP的基本性质之后，考虑MDP中状态转移方程是否具有随机性，以及是否具有时齐性，可以帮助我们确定我们需要的解(最优策略)是什么形式
​		MDP中动作与状态的连续性与随机性对于我们的算法也是很重要的，因为这决定了我们将会采用何种模型；此外，我们还可以考虑MDP是否能把时间也定义为连续的。

## （1）MDP是否发生退化

有的强化学习问题，状态转系关系P发生了退化，可以理解为每一盘游戏只包含一个回合（区别于下象棋）
Example：Multi-armed Bandit(多臂老虎机)、Contextual Bandit(上下文老虎机)

<img src="https://cdn.jsdelivr.net/gh/Lianz-lit/notes-pic/202309022037924.png" style="zoom:50%;" />

​		解决此类问题相比于一般的RL问题更简单，例如针对MAB，只需要与环境不断交互产生数据，估计出哪一个$a$对应的$R_a$的期望最大即可

## （2）环境是否已知

