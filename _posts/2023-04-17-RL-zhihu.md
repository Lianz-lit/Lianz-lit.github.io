---
layout:       post
title:        "强化学习"
author:       "Lianz"
header-style: text
catalog:      true
tags:
    - RL
    - zhihu
---

> 根据知乎用户[@heaven](https://www.zhihu.com/people/yu-xin-hang-22)的系列博客整理（<https://www.zhihu.com/people/yu-xin-hang-22/posts?page=4>）

**目录**

[toc]



# 1.1 什么是强化学习

<img src="https://cdn.jsdelivr.net/gh/Lianz-lit/notes-pic/test_jsdelivr.png" style="zoom:50%;" />

强化学习主要是属于机器学习范畴的一类问题，通常会为这类问题定义以下几个元素：

- 智能体 Agent
- 环境 Environment
- 动作 Action
- 状态 State
- 奖励 Reward

  这些元素之间的关系可以由下图表示：

<img src="https://pic3.zhimg.com/v2-4ac109628bfd404b3fdb4cf1a166cb12_r.jpg" alt="img" style="zoom:50%;" />

最终的目的是训练智能体选取合适的策略来最大化奖励，策略即面对状态该如何采取动作。

需要注意的是：
		当我们提起“强化学习”的时候，往往会用到“深度学习”的方法，并将其称作“深度强化学习”。
		但我们要注意不要混淆二者的含义，要清楚“强化学习”是一类问题，而“深度学习”是一类方法。

# 1.2 强化学习的基本思想

有监督学习和无监督学习：基于已有的数据，去学习数据的分布或蕴含的其他重要信息。
强化学习：不是基于已有数据，而是对一个环境进行学习；且目标不是数据中的信息，而是寻找能在环境中取得更多奖励的方法。

- 监督学习的目标——“弄清楚环境是什么样的”
- 强化学习的目标——“在这个环境中生活得更好”

**强化学习算法主要涉及到两个组成部分：其一是通过与环境交互产生大量的数据，**
															     **其二是利用这些数据去求解最佳策略**

设计强化学习算法时要考虑：计算量；数据效率（产生数据所需的成本）；训练效率（求解最优策略的效率）


## （1）从环境中产生数据

监督学习—— “拥有数据”意味着拥有环境中随机产生的数据。 
强化学习——“拥有环境”意味着可以自主选择与环境交互的方式，从环境中产生我们需要的数据，比随机产生的训练数据包含更多的价值
					   使探索利用平衡发挥作用

越学越强（强化学习）：
对智能体进行初步训练—》有了初步的分辨能力之后—》判断并产生相对更加“重要”的数据
然后用这些数据优化自身的策略—》更准确地判断哪些数据更加“重要”—》更有针对性地从环境中产生更加有价值的数据，
进一步优化自身的策略

​		强化学习的目标是寻找能够在环境中取得较多奖励的策略。如果仅仅依靠给定的数据是难以求解出很好的策略的。只有拥有可交互的环境，才能充分验证我们策略的有效性。

模仿学习（imitation learning）：将强化学习问题转换为有监督学习问题。但实际应用效果很差。
原因——有监督学习目标为优化预测值和真实值的误差。强化学习中，假设训练出来的模型有误差，会导致实际采取的动作有误差，使得下一状态状态也有误差，不断累积，智能体根本无法应对。类似蝴蝶效应
解决方案——DAgger（Dataset Aggregation）。用已经产生的数据进行模仿，对新产生的数据不断进行标注。                                                  <img src="https://pic3.zhimg.com/v2-988795f3f27434444acbd8ea5bf5b582_r.jpg" alt="img" style="zoom: 50%;" />



Note：
拥有环境 VS 拥有数据。前者更好。
但实践中，缺乏可自由交互的环境，只能通过历史数据学习策略，并用于环境中——Offline Reinforcement Learning (或称为batch RL)，

## （2）求解最佳策略

- 基于价值（时间查分的思想，即time difference，TD）
- 基于策略（具有优化思想）

最优控制（optimal control）问题：求解一个环境已知的MDP
														已知环境表达式（S，AR之间的转移关系已知），求解最优策略

Summary：
与环境交互产生数据——> 通过数据求解最佳策略
需要同时兼顾二者，提升数据效率和计算效率 

# 2.1 马尔可夫决策过程

## （1）马尔可夫过程

对于一个马尔可夫过程（一类随机过程），**Xt+1的分布只和Xt的取值有关，和Xt之前发生过的事情无关**。一环扣一环的因果关系，形成马尔可夫链。

<img src="/Users/lit/Library/Application Support/typora-user-images/image-20230606014250177.png" alt="image-20230606014250177" style="zoom:50%;" />

<img src="/Users/lit/Library/Application Support/typora-user-images/image-20230606014406136.png" alt="image-20230606014406136" style="zoom:50%;" />

马尔可夫链的核心性质：如果已知某些条件，要求解某些表达式关于这些条件的期望，啧事实上只需要距离所求事件最接近的条件就足够了。

**一个随机过程能不能用马氏过程来建模，在于能否很好地定义状态以及状态之间的转移方程。**

## （2）马尔可夫决策过程

MP只有状态及状态间的内在关系，在MDP中，**增加行为$A_t$与奖励$R_t$**。

- 增加$A_t$相当于增加“主观能动性”，作为系统输入。
- 增加$R_t$相当于采取行动带来的回报，类似系统输出

使得MDP中状态间的转移关系不再是内部的，而是由状态St与行为At共同决定。

<img src="/Users/lit/Library/Application Support/typora-user-images/image-20230606015707492.png" alt="image-20230606015707492" style="zoom: 33%;" />

$R_t$由$s_t$和$a_t$共同决定，服从概率分布。

MDP中，核心目标就是求出一种选择$A_t$的方式，使获得的奖励最多。

**结束信号 $Done_t$**：代表MDP何时结束

## （3）MP与MDP对比

